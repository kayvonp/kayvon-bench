# Active defaults used when a profile argument is not provided.
active:
  llm_profile: together_qwen3_235b_thinking_private_4xH200
  benchmark_profile: summarization_20000to2500_non_strict

providers:
  openai:
    base_url: https://api.openai.com/v1
    api_key_env: OPENAI_API_KEY
    api_style: responses

  together:
    base_url: https://api.together.xyz/v1
    api_key_env: TOGETHER_API_KEY
    api_style: chat_completions

llm_profiles:
  openai_fast:
    provider: openai
    model: gpt-5-nano

  together_gpt-oss-20b_public:
    provider: together
    model: openai/gpt-oss-20b
    reasoning_effort: high

  together_gpt-oss-20b_private:
    provider: together
    model: kpirestani_0236/openai/gpt-oss-20b-398585b7
    reasoning_effort: high

  together_gpt-oss-120b_private:
    provider: together
    model: kpirestani_0236/openai/gpt-oss-120b-2fefaac0
    reasoning_effort: low

  together_qwen3_235b_thinking_private_4xH200:
    provider: together
    model: together_sso/Qwen/Qwen3-235B-A22B-Thinking-2507-34b36af0
    reasoning_effort: high

  # Template for arbitrary OpenAI-compatible endpoints.
  custom_example:
    provider: custom
    base_url: https://your-endpoint.example/v1
    api_key_env: YOUR_API_KEY_ENV
    api_style: chat_completions
    model: your/model-id

benchmark_profiles:
  summarization_13000to600:
    diagnostic_mode: false
    max_output_tokens_mode: fixed
    max_output_tokens_fixed: 1800
    # Uses active.llm_profile unless explicitly overridden elsewhere.

    # Input dataset (JSONL). Sampling is done with replacement by default.
    dataset_path: corpus/v1_summarization_13000to600.jsonl
    sample_with_replacement: true
    random_seed: 42

    # Keep this quick but meaningful: ramp through selected concurrency levels and scale request volume by concurrency.
    concurrency_levels: [16, 32]
    request_sizing:
      mode: concurrency_scaled
      # Requests per level = max(min_requests, concurrency * waves_per_level),
      # optionally capped by max_requests.
      waves_per_level: 8
      min_requests: 16
      max_requests: 256
    progress_update_every: 3
    request_timeout_s: 90

    # Stop early when throughput plateaus/declines while tail latency spikes.
    stop_on_saturation: true
    min_levels_before_stop: 3
    saturation:
      throughput_plateau_pct: 0.03
      throughput_decline_pct: 0.07
      latency_spike_multiplier: 1.35

  summarization_20000to2500_strict:
    diagnostic_mode: true
    max_output_tokens_mode: fixed
    max_output_tokens_fixed: 7500
    # Uses active.llm_profile unless explicitly overridden elsewhere.

    # Input dataset (JSONL). Sampling is done with replacement by default.
    dataset_path: corpus/v1_summarization_20000to2500_strict.jsonl
    sample_with_replacement: true
    random_seed: 42

    # Keep this quick but meaningful: ramp through selected concurrency levels and scale request volume by concurrency.
    concurrency_levels: [2, 4, 8, 16, 32]
    request_sizing:
      mode: concurrency_scaled
      # Requests per level = max(min_requests, concurrency * waves_per_level),
      # optionally capped by max_requests.
      waves_per_level: 8
      min_requests: 16
      max_requests: 256
    progress_update_every: 3
    request_timeout_s: 90

    # Stop early when throughput plateaus/declines while tail latency spikes.
    stop_on_saturation: true
    min_levels_before_stop: 3
    saturation:
      throughput_plateau_pct: 0.03
      throughput_decline_pct: 0.07
      latency_spike_multiplier: 1.35

  summarization_20000to2500_non_strict:
    diagnostic_mode: false
    max_output_tokens_mode: fixed
    max_output_tokens_fixed: 7500
    # Uses active.llm_profile unless explicitly overridden elsewhere.

    # Input dataset (JSONL). Sampling is done with replacement by default.
    dataset_path: corpus/v1_summarization_20000to2500_non_strict.jsonl
    sample_with_replacement: true
    random_seed: 42

    # Keep this quick but meaningful: ramp through selected concurrency levels and scale request volume by concurrency.
    concurrency_levels: [2, 4, 8, 16, 32]
    request_sizing:
      mode: concurrency_scaled
      # Requests per level = max(min_requests, concurrency * waves_per_level),
      # optionally capped by max_requests.
      waves_per_level: 8
      min_requests: 16
      max_requests: 256
    progress_update_every: 3
    request_timeout_s: 90

    # Stop early when throughput plateaus/declines while tail latency spikes.
    stop_on_saturation: true
    min_levels_before_stop: 3
    saturation:
      throughput_plateau_pct: 0.03
      throughput_decline_pct: 0.07
      latency_spike_multiplier: 1.35
